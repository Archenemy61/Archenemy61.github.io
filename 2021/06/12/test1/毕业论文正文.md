#  毕业论文正文

## 1  绪论

### 1.1 课题的研究背景和意义

​		随着互联网在中国的飞速发展，尤其是在web2.0出现后。网站与用户之间的双向交流逐渐增强，互联网上的各种信息出现爆炸式的增长，相较于之前，越来越多的用户倾向于在互联网上发表自己的观点与评论，分享自己的想法与经验，表达自己的情感。因此互联网上便出现了大量的主观性文本，这些文本代表了用户的想法与情感倾向。例如，对于近期上映的电影或出版的图书，在一些门户网站上便会出现供用户针对其讨论的版块。电子商城的用户评论区会有大量的主观性评价，在某一条新闻的评论区也会出现大量的用户观点与评价。对于上述主观性文本，采用计算机收集并研究分析其中表达的情感成为了近年来学术界研究的一个热点，这被称为文本情感分析。

​		近年来，文本情感分析逐渐成为了自然语言处理研究领域中的重要课题，也被称为意见挖掘，情感分析是对用户发表在各大门户网站上的主观性文本进行分析，对其观点、态度和评论以及针对产品、服务和新闻事件的情感倾向做出有效的分析而后进行归纳和推理的技术。情感分析包含着许多复杂的问题，同时也意味着有许多不同的研究内容，根据处理文本的粒度可以将其分为词语级、短语级、文章级。根据文本的类型可以分为舆情情感分析和评价情感分析。

​		判别词语情感倾向性是情感分析中最为基础的工作，因此基于情感词典的情感分析是一种得到广泛应用的方法。结合情感词典对文本进行特征抽取。然而互联网上的文本具有观点表达较为隐晦，口语色彩浓重以及不规范性等特点，所以采用传统的情感词典无法对某些特定的网络评论进行较为准确的情感分析。因此需在机器学习的基础上采用半监督学习，利用收集到的语料对模型进行训练，再与已有的情感词典结合构建本文的情感词典，可以大大提高对如今网络上的主观性文本进行情感识别的准确率。

​		目前在情感分析中主要采用两种方法，第一种是基于情感词典的方法，统计待分析文本中被情感词典划分为正向词与负向词的个数，从而计算出每个句子的评分来判断句子的情感倾向。另一种是基于机器学习的方法，利用训练语料库，使用支持向量机（Support Vector Machine），朴素贝叶斯，K临近（k-NearestNeighbor)等分类算法进行情感分类。中文与英文具有非常大的区别，首先中文语句重视语义，英文语句重视结构。所以不同语序的中文语义也会发生翻天覆地的变化，因此对于英文语句所适用的模型对于中文语句并不适用。与此同时，由于汉语存在缺乏严格意义上的形态变化，所以此词类和成分并不像英文那样一一对应，因此基于英文情感词典的情感分析准确率要远高于基于中文情感词典的情感分析。 

### 1.2研究现状及分析

​		情感分析的概念首先由R.Picard 教授在1995年提出，她在其专著《Affective Computing》中将情感分析定义为“情感计算是与情感相关、来源于情感或能对情感施加影响的计算”。情感分析近年来成为人工智能方面的热点问题。目前在处理文本情绪分析问题中有两种不同的方向，一种是基于规则的方向，消耗大量的精力集中于分析规则的制订，并且对于语言现象非常复杂的中文来说，规则的制订显得十分艰难。另一种方向则是基于统计学习，也是目前大多数学者所采用的方向，通过统计学习语句的和词语的特征，根据特征的分布对文本做出情感分析。

​		情感分析可划分为情感信息分类和情感信息抽取，根据研究文本的粒度可分为词语级，语句级和篇章级的情感分析。词语级情感分析是研究文本情感分析的重要基础，为了定义词语的情感极性，利用某个实数对词语的情感极性进行评估，其中大于0表示情感倾向于正面，小于0则倾向于负面。而情感极性词语中大多以名词、动词、形容词和副词为主。所以可以通过查询情感词典的方式获取词语的情感极性。对于未被情感词典收录的词语的极性则可以通过语料库的方式获取。语句级情感分析的处理对象则是整个句子，通过分析句子结构与句式和主客体来将所有词汇整体建模进行情感倾向性分析，一般采用基于词典的方法和基于机器学习的方法。篇章级情感分析从整体上分析某个文章的褒贬态度可以分为基于统计自然语言处理模型的分类方法和基于语义理解的情感识别方法。然而目前对于篇章级的情感分析集中用于对产品评论的分析，而且要求句式规范，结构完整并且无明显语法错误，显然对于当今网络中的短文本并不适用。因此对于网络短文本的情感分析主要集中在词语级和句子级情感分析的应用。

### 1.3 本文的主要研究内容

​		本文的主要研究内容分为三个部分：数据清洗与预处理，数据向量化与特征化，模型训练与评分。研究中首先选择训练数据集，本文选择的数据集为外卖评论数据集，豆瓣电影评论数据集及带有情感极性的微博数据集。分别针对不同类型的文本进行模型的构建。然后进行数据清洗与数据预处理。将预处理后的数据进行特征化和向量化，将数据转化为训练集，最终分别利用支持向量机（Support Vector Machine），朴素贝叶斯和K临近（k-NearestNeighbor)等模型对训练集进行训练和评分，分析对比不同模型以及训练集的得分，选择最佳的模型。整体的设计方案如图1-1所示：

![image-20200421154241096](https://gitee.com/archenemy/images/raw/master/img/image-20200421154241096.png)

​		由于所收集到的数据是利用网络爬虫直接从门户网站爬取得到的，因此数据中包含一些与本研究无关的信息，例如微博文本中的图片链接，用户名以及视频链接，外卖评论与豆瓣电影评论中的表情符号等。所以对原数据进行清洗则显得至关重要，本文中选择使用python语言的pandas库读取数据文件并利用正则表达式去除上述信息。在数据预处理中要将原数据随机划分为训练集以及测试集，利用训练集对模型进行训练，测试集对训练好的模型测试评分。

​		在特征化与向量化阶段，由于训练集所得到的中文文本无法直接进行训练，因此需要将中文转化为可以进行训练的数据类型。本文将中文语句进行分词后提取特征转化为特征向量，再利用词语的特征向量合成后转化为句子的特征向量，经过特征化与向量化处理后，一句中文语句便可转化成为一个200-300维度的行向量。

​		训练阶段中，本文采用当前处理自然语言最为广泛的三种模型进行训练，分别为支持向量机（Support Vector Machine），朴素贝叶斯和K临近（k-NearestNeighbor)模型，分别对三种模型的训练结果使用相同的测试集进行测试，得到模型评分以及拟合曲线，分析对比不同模型得出结论。

## 2  情感分析相关概念

### 2.1文本情感分析的过程

​			中文的文本情感分析涉及到许多领域，包括自然语言处理，数据挖掘，人工智能，机器学习等多个领域。如图2-1所示，典型的情感分析包含以下几个步骤。

![image-20200422151129394](https://gitee.com/archenemy/images/raw/master/img/image-20200422151129394.png)

1. 收集原始语料库，目前由于缺乏公开的用语中文文本情绪分析的语料库，因此需要从门户网站，豆瓣电影平台，各大论坛等一些站点来手机语料。

2. 语料预处理。由于网络用语的不规范性以及口语性，需要对收集到的语料进行预处理，通常的预处理主要包括去除停用词、分词、词性标注、词缀修剪等。
3. 识别主观句子。情感词是包含用户评价的词语，对于情感分析十分重要，因此需要寻找句子中的情感词。找寻含有情感词的主观句子。
4. 特征提取。在进行分类之前要进行特征提取，最直接的方法是直接抽取代表情感的词语，例如“好吃”，“推荐”，“好评”等。通过构建情感词汇表来抽取。
5. 情感分类。一般采用基于机器学习的标准分类器，最常用的分类器是支持向量机（Support Vector Machine）和朴素贝叶斯分类器。

### 2.2 互联网短文本的研究和概述

#### 2.2.1 互联网短文本的概念

​		互联网短文本是指互联网中由用户所发出的篇幅较短的文本形式，例如微博和评论等。一般不多于140字。这类文本由于是用户针对某一话题而发出的，所以一般具有强烈的主观情感，然而这些主观性文本也有着对于情感分析的难点，因此需要研究互联网短文本的特点并采取合适的处理方法。互联网短文本具有以下特点

1. 稀疏性：短文本的内容较短，通常只包含几个到十几个有实际意义的词语，难以抽取有效的特征词
2. 实时性：短文本更新速度快、易于扩散。
3. 海量性：短文本大量存在于人们的生活中，由于短文本的及时更新和快速传播，使互联网中积累了海量的短文本数据，这要求对于短文本的处理计算必须具有很高的速度。
4. 不规范性：短文本表述简洁，简称、不规范用语以及网络流行用语被广泛使用，使文本噪音较大。如“天朝”－网络用语，“杯具”－谐音用法，“666”－新词汇。

短文本的特点使文本分类面临以下难点：

1. 短文本特征词少，用传统的基于词条的向量空间模型表示，会造成向量空间的稀疏。另外，词频、词共现频率等信息不能得到充分利用，会丢失掉了词语间潜在的语义关联关系。
2. 短文本的不规范性，使文本中出现不规则特征词和分词词典无法识别的未登录词，导致传统的文本预处理和文本表示方法不够准确。 
3. 短文本数据的规模巨大，在分类算法的选择上往往更倾向于非惰性的学习方法，避免造成过高的时间复杂度。
   因此，短文本分类一般在预处理、文本表示、分类器的构建等环节中进行优化和改进，以提高分类效果和精度。

#### 2.2.2 互联网短文本的研究资源

​		互联网文本情感分析需要大规模语料库的支持，随着互联网在我国的高速发展，用户开始发表大量的主观性文本，如影评、商品评价，微博等。这些主观性文本为互联网短文本的研究提供了丰富的素材，这是因为这些评论类的资源处于公开状态，易于通过爬虫等方法大批量获取，而且这些文本大多数都是主观性文本，带有一定的感情色彩，不再需要主观性句子的识别过程，简化了情感分析的流程。

### 2.3 词向量  

#### 2.3.1 词的表示

​		在自然语言处理中首先要考虑词如何在计算机中表示，通常情况下有两种表示方式：离散表示（one-hot representation）和分布式表示（distribution representation）。

（1)离散表示(one-hot representation)

​		传统的基于规则的或者基于统计的自然语言处理方法将单词看做符号，称作one-hot representation，这种方法将每一个词表示为一个长单位向量。其中向量的维度是单词表的长度，向量中只有一个维度为1，其余维度均为0，这个向量代表了当前词。例如“好看”和“漂亮”可以表示为：

好看：[1, 0, 0, 0, 0, 0, 0,...]

漂亮：[0, 0, 0, 0, 1, 0, 0,...]

​		从以上两个向量可以看出离散表示(one-hot representation)存在两个问题

1. “词汇鸿沟”问题。离散表示法中每个词都是孤立的，无法判断词与词之间的联系，例如上例中的“好看”与“漂亮”，即使词义相关，但是无法通过向量分辨出来。
2. “维度灾难”问题。由于对于中文来说，词汇的数目十分庞大，即使在一篇文章中出现的词汇数量也比较多，要将词汇离散表示所需要的维度也随之变得很高，无论是对于机器学习还是深度学习都无法承担如此高维度的向量。

（2）分布式表示(distribution representation)

​		相较于离散表示法，将词转化为词向量，也就是分布式表示能够很好地解决上述问题。词向量的维度都是预先定义的，每一维度上均为实数，因此既可以区分不同词语，也可以通过向量之间的计算来表示词语之间的联系。如：

好看：[1.58, 0.25, 5.69, -1.35, 5.31, -0.15, 4.56,...]

漂亮：[1.67, 0.46, 5.88, -1.27, 5.42, -0.54,  4.80...]

​		分布式表示方法具有以下优点

1. 词之间存在相似关系，通过向量之间的距离来表示词之间的相似关系，避免了“词汇鸿沟”问题，对于自然语言处理十分有帮助。

 	2. 在相同维数下可以比one-hot representation表示更多词汇，避免了“维度灾难”问题。

#### 2.3.2 词向量的生成

​		生成词向量的方法有很多，大多都遵循同一个思想：任意词的含义可以通过其周边词来表示，生成词向量的方式可分为：基于统计的方法和基于语言模型(language model)的方法。

（1）基于统计的方法

  1. 使用共现矩阵

     通过统计一个事先指定大小的窗口内的word共现次数，以word周边的共现词的次数做为当前word的vector。具体来说，我们通过从大量的语料文本中构建一个共现矩阵来定义word representation。例如有语料如下：

     我喜欢深度学习

     我喜欢自然语言

     我喜欢读书

     则其共现矩阵如下：其中“我”与“喜欢”在每句话中都相邻，则值为3

     |      | 我   | 喜欢 | 深度 | 学习 | 自然 | 语言 | 读书 |
     | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
     | 我   | 0    | 3    | 0    | 0    | 0    | 0    | 0    |
     | 喜欢 | 3    | 0    | 1    | 0    | 1    | 0    | 1    |
     | 深度 | 0    | 1    | 0    | 1    | 0    | 0    | 0    |
     | 学习 | 0    | 0    | 1    | 0    | 1    | 0    | 0    |
     | 自然 | 0    | 1    | 0    | 0    | 0    | 1    | 0    |
     | 语言 | 0    | 0    | 0    | 0    | 1    | 0    | 0    |
     | 读书 | 0    | 1    | 0    | 0    | 0    | 0    | 0    |

     矩阵定义的词向量在一定程度上缓解了one-hot向量相似度为0的问题，但没有解决数据稀疏性和维度灾难的问题。

     2. 奇异值分解（SVD)

        基于共现矩阵得到的词向量依旧存在稀疏和高维度的问题，因此需要对原始词向量进行降维得到一个稠密的连续词向量。对词向量进行SVD分解得到正交矩阵U，对U归一化后即可得到低维度稠密矩阵。该矩阵具有很多良好的性质：语义相近的词在向量空间相近，甚至可以一定程度反映word之间的线性关系。

（2） 基于语言模型的方法

​		语言模型。 语言模型的目的是为语句的联合概率函数P(w1,...,wT)建模, 其中wi表示句子中的第i个词。语言模型的目标是，对于模型有意义的句子赋予大概率。 这样的模型可以应用于很多领域，如机器翻译、语音识别、信息检索、词性标注、手写识别等，它们都希望能得到一个连续序列的概率。 以信息检索为例，当你在搜索“自然语年处理”时，搜索引擎会提示你是否希望搜索"自然语言处理", 这是因为根据语言模型计算出“自然语年处理”的概率很低，而与语年近似的，可能引起错误的词中，语言会使该句生成的概率最大。

​	对语言模型的目标概率P(w1,...,wT)，如果假设文本中每个词都是相互独立的，则整句话的联合概率可以表示为其中所有词语条件概率的乘积，即：

![image-20200425114723386](https://gitee.com/archenemy/images/raw/master/img/image-20200425114723386.png)



然而语句中的每个词出现的概率都与其前面的词相关, 所以实际上通常用条件概率表示语言模型：

![image-2020025114723386](https://upload-images.jianshu.io/upload_images/1803066-42b8d29d5384ce54.png)

以上简单介绍语言模型，在下一章会详细介绍本文所采用的word2vec模型。

## 3 数据清洗与预处理

### 3.1 数据源介绍

​	 	由于目前用于中文自然语言处理的语料库十分有限，并且大多都是在2010年左右所收集，根据互联网语言的特性以及互联网大环境对比，可以知道当时的互联网语言与现在的互联网语言完全不同，例如“雷人”一次在当时十分流行，但如今确很少有人使用。并且当时外卖以及电商产业并没有崛起，因此也没有产品评价的数据集，综上所述，本文使用了中文自然语言研究论坛中提供的近几年来使用爬虫爬取到的自于不同类型网站的主观性文本以及评价并带有情感极性标注。分别是外卖评论数据集，豆瓣电影评论数据集以及微博数据集。

外卖评论数据集如下表所示

![image-20200426180444017](https://gitee.com/archenemy/images/raw/master/img/image-20200426180444017.png)

共11987条数据，review列代表评论的主观性文本，label列代表情感倾向，其中0代表负向，1代表正向。

豆瓣电影评论数据源如下表所示

![image-20200426180846607](https://gitee.com/archenemy/images/raw/master/img/image-20200426180846607.png)

电商评论数据集包括2125056条数据，comment列代表主观性文本，rating表示用户评分，本文中将评分低于3分的评论视为负面评论，评分高于3分的评论视为正面评论。

带有情感标注的微博数据集如下

![image-20200426181554352](https://gitee.com/archenemy/images/raw/master/img/image-20200426181554352.png)

微博数据集中包含119988条微博，其中review列为微博正文，label列为情感倾向，0代表正向，1代表负向。

### 3.2 数据清洗

​		从上文中可以看出原始数据中存在许多与本文无关的影响因素，如微博数据集中的@后的昵称，标点符号，英文，以及一些表情符号。同时由于中文中存在许多停用词，如“的”、“有点”、“啦”等与句意无关的词汇，因此也需要去除。本文采用正则表达式去除标点符号以及一些无意义的符号，处理过程：

```python
	for neg_word in neg_words:
    neg_word = neg_word.strip()
    neg_word = re.sub(r"[0-9\s+\.\!\/_,$%^*()?;；:-【】+\"\']+|[+——！，;:。？、~@#￥%……&*（）]+", " ", neg_word)
```



![image-20200427144520770](https://gitee.com/archenemy/images/raw/master/img/image-20200427144520770.png)

可以看出已经将每一句的标点符号换成了空格





### 3.3 数据预处理

#### 3.3.1 文本分词

​	本文采用结巴分词对主观性文本进行分词，结巴分词是一个开源的python中文分词组件，支持四种分词模式，目前大多数的中文自然语言处理都采用此组件进行分词。分词的结果中包含“于是”，“仅仅”，“看上去”等并没有实际意义的词汇，称为停用词，因此需要使用停用词表对词汇进行过滤并提出，本文中使用四川大学机器智能实验室停用词库来作为停用词表。

（1）结巴分词的特点

1、支持三种分词模式：
　　(1)精确模式：试图将句子最精确的切开，适合文本分析。
　　(2)全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。
　　(3)搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
2、支持繁体分词
3、支持自定义词典

（2） 结巴分词的实现原理

(1)基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)。
(2)采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合。
(3)对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。

由于本文仅使用了结巴分词中的精简分词功能，因此对于其原理不再赘述。

（3）加载停用词表

```python
words = pd.read_csv('waimai_10k.csv.txt')
neg = words[words['label']==0]
f=open("scu_stopwords.txt","r")
stopwords={}.fromkeys(f.read().split("\n"))
f.close()
```



（4）分词并过滤停用词

```python
for neg_word in neg_words:
    outStr = ""
    neg_word = neg_word.strip()
    seg_list=jieba.cut(neg_word,cut_all=False)  #结巴分词
    for word in seg_list:
        if word not in stopwords:
            outStr+=word
            outStr += " "
    res.append(outStr)
```

![image-20200427144752268](https://gitee.com/archenemy/images/raw/master/img/image-20200427144752268.png)

#### 3.3.2 词向量化

​	由于中文无法被计算机语言直接使用，同时占用空间大，无法作为训练集，因此需将所有词语转化为向量，以便于进行拟合与训练。在此采用word2vec将词语转化为向量。

（1）word2vec原理

​	word2vec是google在2013年提出NLP模型，它的特点是将所有的词表示成低维稠密向量，从而可以在词向量空间上定性衡量词与词之间的相似性。越相似的词在向量空间上的夹角会越小。word2vec使用的模型是简单化的神经网络，如下图所示：

![image-20200428170146324](https://gitee.com/archenemy/images/raw/master/img/image-20200428170146324.png)

​		输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，并不会用这个训练好的模型处理新的任务，真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。

​		此模型定义的输入和输出一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。Skip-Gram模型和CBOW的思路相反，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好。

（2） Continuous Bag-of-Words (CBOW) 模型

![](https://gitee.com/archenemy/images/raw/master/img/cbow.jpg)·

​		CBOW 给定目标单词的上下文（前c个词以及后c个词）预测该目标单词是什么，可以用条件概率来建模这个问题，所以，该模型是求：

![image-20200428170611019](https://gitee.com/archenemy/images/raw/master/img/image-20200428170611019.png)



对于给定的一句话w1,w2,w3...wT，该模型的目标函数就是最大化上式的对数似然函数：

![image-20200428170627408](https://gitee.com/archenemy/images/raw/master/img/image-20200428170627408.png)

- T: 句子长度
- wt: 要预测的目标单词
- c: 上下文大小

条件概率由 softmax 给出：

![image-20200428170659342](https://gitee.com/archenemy/images/raw/master/img/image-20200428170659342.png)



（3）Skip-gram模型

![](https://gitee.com/archenemy/images/raw/master/img/skgrim.jpg)

skip-gram 模型与 CBOW 恰好反过来，它是在给定一个单词的条件下，预测其上下文单词最有可能是哪些。

给定一句话`w1,w2...wT`该模型的目标函数：

![image-20200428171451674](https://gitee.com/archenemy/images/raw/master/img/image-20200428171451674.png)

在用梯度下降对 L 求导时，计算代价正比于整个单词表大小 N 。对此，常见的优化手段包括：

- Hierarchical softmax: 不是直接计算softmax，而是通过构建一颗二叉树，把问题转化为 Log N 次二分类问题。spark 上 word2vec 实现采用此方法。
- Negative sampling： 负采样，完全抛弃了 softmax，从词汇表随机采样 neg 个(context(w),wi)构成负样本，训练里存在的(context(w), w)构成正样本，将多分类问题转化为 neg 个二分类问题。tensorflow 上的 word2vec 是负采样实现的。

（4）预处理结果

 ```python
from gensim.models.word2vec import Word2Vec
w2v = Word2Vec(size=300, min_count=10)
w2v.build_vocab(x)
w2v.train(x, total_examples=w2v.corpus_count, epochs=w2v.iter)
 ```

​		转化后将词语“好吃”所转化的向量打印出来如下表

![](https://gitee.com/archenemy/images/raw/master/img/word2vecResult.png)

​		可以看出“好吃”转化为一个200维的行向量。根据word2vec的性质可知每一个词汇都与其在句子中相邻位置的词汇有关，因此将测试集中的句子所获得的向量线性合成为句向量，方便训练集进行训练。

```python
def total_vec(wordset):
    vec = np.zeros(300).reshape((1,300))
    for word in wordset:
        try:
            vec += w2v[word].reshape((1,300))
        except KeyError:
            continue
    return vec
```

​		至此为止，用于模型训练所需的训练集和测试集已经处理完毕。接下来便是构建文本情感分类器并使用训练集进行训练。



## 4 基于SVM算法的文本情感分类器

### 4.1 支持向量机（support vector machine）概述

​		支持向量机是找到一个超平面（hyperplane）将数据划分为几类的一种二分类模型，通常适用于可直接分为两类的数据，高维不能线性可分的数据以及简单分类。本文中用于分类的数据为高维向量且通常只有两类——情感正向与情感负向。并且大量研究表明，支持向量机模型在解决非线性、高维模式识别中具有许多特有的优势，但是其时间复杂度较高。训练过程的基本思想是找到一个用向量w所表示的超平面，在使数据区分或边缘最大化的基础上将向量区别于其他类。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

### 4.2 算法设计与分析

#### 4.2.1 划分训练集以及测试集

（1）训练集与测试集

​		模型训练的目的是将训练好的模型部署到真实的环境中，希望训练好的模型能够在真实的数据上得到好的预测效果，换句话说就是希望模型在真实数据上预测的结果误差越小越好。我们把模型在真实环境中的误差叫做泛化误差，最终的目的是希望训练好的模型泛化误差越低越好。我们希望通过某个信号来了解模型的泛化误差，这样就可以指导我们得到泛化能力更强的模型：

1. 使用泛化误差本身。这是很自然的想法，我们训练模型的最终目的就是希望模型的泛化误差最低，当然可以使用泛化误差本身来作为检测信号。如果泛化误差小的话还可以接受，但是通常情况下没有那么幸运，泛化误差可能很大，这个时候你肯定会将部署的模型撤回，重新训练，你可能需要部署和训练之间往复很多次，这种方式虽然能够更好的指导我们的模型，但是成本和效率非常的差；

    		2. ​     使用模型在数据集上训练的拟合程度来作为评估模型的信号。但是往往我们获取的数据集并不是完全的干净以及有代表性，通常我们获取到的数据集可能很少、数据的代表性不够、包含太多的噪声或者是被一些无关特征污染，我们获取到的数据集或多或少都会有这些问题，那么模型对训练数据集的拟合程度不能指导泛化误差，也就是说训练的时候拟合的好并不代表模型的泛化误差就小，你甚至可以将模型在数据集上的误差减小到0，但是因为对模型训练时候的数据集往往不干净，所以这样的模型并不代表泛化能力就强。

​     模型验证中既不能通过直接将泛化误差作为了解模型泛化能力的信号，因为在部署环境和训练模型之间往复，代价很高，也不能使用模型对训练数据集的拟合程度来作为了解模型泛化能力的信号，因为我们获得的数据往往不干净。

​     更好的方式就是将数据分割成两部分：训练集和测试集。我们可以使用训练集的数据来训练模型，然后用测试集上的误差作为最终模型在应对现实场景中的泛化误差。有了测试集，我们想要验证模型的最终效果，只需将训练好的模型在测试集上计算误差，即可认为此误差即为泛化误差的近似，我们只需让我们训练好的模型在测试集上的误差最小即可。

（2）划分过程

​		本文中使用机器学习框架sklearn所提供的train_test_split方法对预处理后的数据集进行区分。该方法接受4个参数包括：train_data：所要划分的样本特征集；train_target：所要划分的样本结果；test_size：样本占比，如果是整数的话就是样本的数量；random_state：随机数的种子。以下是划分结果。

```python
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(train_vec,y,test_size=0.2,random_state=0)
X_train.size = 9589
Y_train.size = 2398
X_test.size = 9589
Y_test.size = 2398
```

​		可以看出本次划分采用测试集占比20%，得到2398个测试集以及9589个训练集。

### 4.2.2 模型训练以及结果分析

​		使用SVM模型中的SVC函数对训练集进行训练，该函数包含以下参数

1. C ：惩罚参数C，C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。本文采用默认值1.0。

2. kernel：核函数，默认是rbf，可以是‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ ，这里采用rbf

3. 是否允许冗余输出，这里允许。

   ​	其余参数均保持模型默认值，以下是代码：

```python
from sklearn.svm import SVC
model = SVC(kernel = 'rbf', verbose=True)
model.fit(X_train, Y_train)
SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=True)
```

​			训练后的模型包含很多参数，





































## 5 基于KNN算法的文本情感分类器

### 5.1 K临近（KNN）模型概述



​		 最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。

   		KNN是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

​		在KNN中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离：

![image-20200429190830222](https://gitee.com/archenemy/images/raw/master/img/image-20200429190830222.png)

​		同时，KNN通过依据k个对象中占优的类别进行决策，而不是单一的对象类别决策。这两点就是KNN算法的优势。

​		接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

1）计算测试数据与各个训练数据之间的距离；

2）按照距离的递增关系进行排序；

3）选取距离最小的K个点；

4）确定前K个点所在类别的出现频率；

5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

### 5.2 算法实现







## 6 基于朴素贝叶斯算法的文本情感分析器

### 6.1 贝叶斯分类概述

#### 6.1.1 贝叶斯决策论

​		贝叶斯分类器是一类分类算法的总称，贝叶斯定理是这类算法的核心，因此统称为贝叶斯分类。贝叶斯决策论通过**相关概率已知**的情况下利用**误判损失**来选择最优的类别分类。

　　“风险”(误判损失)= 原本为cj的样本误分类成ci产生的期望损失，期望损失可通过下式计算：

![image-20200429191417351](https://gitee.com/archenemy/images/raw/master/img/image-20200429191417351.png)

​		为了最小化总体风险，只需在每个样本上选择能够使条件风险R(c|x)最小的类别标记。最小化分类错误率的贝叶斯最优分类器为：

![image-20200429191455061](https://gitee.com/archenemy/images/raw/master/img/image-20200429191455061.png)

​		即对每个样本x，选择能使后验概率P(c|x)最大的类别标记。

​	 利用贝叶斯判定准则来最小化决策风险，首先要获得后验概率P(c|x)，机器学习要实现的是基于有限的训练样本集尽可能准确的估计出后验概率P(c|x)。主要有两种模型：一是“判别式模型”：通过直接建模P(c|x)来预测，其中决策树，BP神经网络，支持向量机都属于判别式模型。另外一种是“生成式模型”：通过对联合概率模型P(x，c)进行建模，然后再获得P(c|x)。对于生成模型来说：

​		![image-20200429191545226](https://gitee.com/archenemy/images/raw/master/img/image-20200429191545226.png)

​		基于贝叶斯定理，可写为下式（1）

![image-20200429191609548](https://gitee.com/archenemy/images/raw/master/img/image-20200429191609548.png)

​		P(c)是类“先验”概率，P(x|c)是样本x相对于类标记c的类条件概率，或称似然。p(x)是用于归一化的“证据”因子，对于给定样本x，证据因子p(x)与类标记无关。于是，估计p(c|x)的问题变为基于训练数据来估计p(c)和p(x|c)，对于条件概率p(x|c)来说，它涉及x所有属性的联合概率。

#### 6.1.2  朴素贝叶斯分类器

​		基于贝叶斯的公式（1）来估计后验概率p(c|x)的主要困难在于：条件概率p(x|c)是所有属性上的联合概率，难以从有限的训练样本直接估计而得。朴素贝叶斯采用了“属性条件独立性假设”可以避开这个问题，意思是：假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。基于属性条件独立性假设，式（1）可重写为：	

![image-20200429191817352](https://gitee.com/archenemy/images/raw/master/img/image-20200429191817352.png)



​		其中，d为属性数目，xi为x在第i个属性上的取值。

​		由于对于所有的类别p(x)相同，基于（2）式的贝叶斯判定准则有

​		![image-20200429191843490](https://gitee.com/archenemy/images/raw/master/img/image-20200429191843490.png)

​		这就是朴素贝叶斯分类器的表达式。

### 6.2 模型训练以及结果分析











## 7 总结与展望



